{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "c08e9d85bc044d669de2bdedf1c8f376": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_bebcc0d9c09f46f0967f577e22e25344",
              "IPY_MODEL_6826b435ddb94e139dc8a91d9b8dab85"
            ],
            "layout": "IPY_MODEL_eee45f78dd264c7a8e2bd23b6fd4ca89"
          }
        },
        "bebcc0d9c09f46f0967f577e22e25344": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_173b2386cbab4a51b710e33f2e4aa5e3",
            "placeholder": "​",
            "style": "IPY_MODEL_c60c5987fb3843f5808ae456e6abb7f8",
            "value": "0.072 MB of 0.072 MB uploaded\r"
          }
        },
        "6826b435ddb94e139dc8a91d9b8dab85": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0e130898c25e4c5eb135ccffa70fd3f9",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_97735ef7ff2c41ed9786057154790244",
            "value": 1
          }
        },
        "eee45f78dd264c7a8e2bd23b6fd4ca89": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "173b2386cbab4a51b710e33f2e4aa5e3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c60c5987fb3843f5808ae456e6abb7f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0e130898c25e4c5eb135ccffa70fd3f9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "97735ef7ff2c41ed9786057154790244": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "! git clone https://github.com/yash-srivastava19/attention-free-revolution.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6qqNKn3wxzyl",
        "outputId": "1dcf54be-aaa7-4ce9-c8d0-085498dffd55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'attention-free-revolution'...\n",
            "remote: Enumerating objects: 82, done.\u001b[K\n",
            "remote: Counting objects: 100% (82/82), done.\u001b[K\n",
            "remote: Compressing objects: 100% (78/78), done.\u001b[K\n",
            "remote: Total 82 (delta 27), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (82/82), 30.55 KiB | 665.00 KiB/s, done.\n",
            "Resolving deltas: 100% (27/27), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install wandb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Je2_7pbbx3Me",
        "outputId": "e8a78f87-6a37-4cdc-c800-5f9a51d042c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wandb\n",
            "  Using cached wandb-0.16.2-py3-none-any.whl (2.2 MB)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Collecting GitPython!=3.1.29,>=1.0.0 (from wandb)\n",
            "  Downloading GitPython-3.1.41-py3-none-any.whl (196 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.4/196.4 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.31.0)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Collecting sentry-sdk>=1.0.0 (from wandb)\n",
            "  Downloading sentry_sdk-1.40.0-py2.py3-none-any.whl (257 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m257.5/257.5 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docker-pycreds>=0.4.0 (from wandb)\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.1)\n",
            "Collecting setproctitle (from wandb)\n",
            "  Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (67.7.2)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.4.4)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Collecting gitdb<5,>=4.0.1 (from GitPython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2023.11.17)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: smmap, setproctitle, sentry-sdk, docker-pycreds, gitdb, GitPython, wandb\n",
            "Successfully installed GitPython-3.1.41 docker-pycreds-0.4.0 gitdb-4.0.11 sentry-sdk-1.40.0 setproctitle-1.3.3 smmap-5.0.1 wandb-0.16.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import math\n",
        "import torch\n",
        "import wandb\n",
        "import warnings\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from scipy.signal import correlate"
      ],
      "metadata": {
        "id": "gMBYcvp9yPBv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('data.txt', 'r', encoding='utf-8') as f:   ## add path to the dataset here.\n",
        "  text = f.read()\n",
        "\n",
        "chars = sorted(list(set(text)))  # possible elements from the dataset.\n",
        "vocab_size = len(chars)\n",
        "\n",
        "print(vocab_size)\n",
        "\n",
        "# Tokenize the set(Character level tokenizer)\n",
        "string_toInt = {ch: i for i, ch in enumerate(chars)}\n",
        "int_toString = {i: ch for i, ch in enumerate(chars)}\n",
        "\n",
        "# Forward and Reverse Mapping(String to Integer). There are different schemes, Google uses sentencepiece,\n",
        "# OpenAI uses tiktoken\n",
        "encode = lambda string: [string_toInt[c] for c in string]\n",
        "decode = lambda lst: ''.join([int_toString[i] for i in lst])\n",
        "\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "N = int(len(data)*0.9)\n",
        "\n",
        "print(N)\n",
        "\n",
        "train_data = data[N:]\n",
        "val_data = data[:N]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c67nA5Wc0a-P",
        "outputId": "7663df05-d3dd-4397-8e9e-678aea322db5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "91\n",
            "4912379\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" Maybe we'll move this in the same file as well, or modify the function a lil bit(much more likely). \"\"\"\n",
        "\n",
        "\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - training_config.block_size, (training_config.batch_size,))\n",
        "    x = torch.stack([data[i:i+training_config.block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+training_config.block_size+1] for i in ix])\n",
        "    x, y = x.to(training_config.device), y.to(training_config.device)\n",
        "    return x, y\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(training_config.eval_iters)\n",
        "        for k in range(training_config.eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n"
      ],
      "metadata": {
        "id": "QoLxEbAjzSaa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BaseLeviathanConfig:\n",
        "    attn_dropout = 0.1\n",
        "    embed_dropout = 0.1\n",
        "    ff_dropout = 0.1\n",
        "\n",
        "    def __init__(self, vocab_size, max_len, **kwargs) -> None:\n",
        "        self.vocab_size = vocab_size\n",
        "        self.max_len = max_len\n",
        "\n",
        "        for key, value in kwargs.items():\n",
        "            setattr(self, key, value)\n",
        "\n",
        "\n",
        "class LeviathanModelConfig(BaseLeviathanConfig):\n",
        "    num_heads = 2\n",
        "    num_blocks = 2\n",
        "    embed_dim = 4\n",
        "\n",
        "leviathan_model_config = LeviathanModelConfig(vocab_size, max_len=1000)     ## Todo: Change these or something like that.\n",
        "\n",
        "class TrainingConfig:\n",
        "    batch_size = 64         # how many independent sequences will we process in parallel?\n",
        "    block_size = 512        # what is the maximum context length for predictions?\n",
        "    max_iters = 10000        # How many epochs the model should be trained for?\n",
        "    eval_interval = 100     # After how many intervals should we see the validation loss?\n",
        "    ckpt_interval = 500     # After how many intervals should we save the model?\n",
        "    learning_rate = 3e-4    # learning rate.\n",
        "    eval_iters = 200\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "training_config = TrainingConfig()\n",
        "print(training_config.device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SCsC4VoyywGi",
        "outputId": "37801ee7-448c-4aff-9a33-68c4b29ed40f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadedCorrelation(nn.Module):\n",
        "    def __init__(self, d_model, num_heads) -> None:\n",
        "        super().__init__()\n",
        "        assert d_model % num_heads == 0\n",
        "\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.d_k = d_model // num_heads\n",
        "\n",
        "        self.W_q = nn.Linear(d_model, d_model)\n",
        "        self.W_k = nn.Linear(d_model, d_model)\n",
        "        self.W_v = nn.Linear(d_model, d_model)\n",
        "        self.W_o = nn.Linear(d_model, d_model)\n",
        "\n",
        "    def scaled_correlation(self, Q, K, V, mask=None, dropout=None):\n",
        "        \"\"\" [NEW: This is what is different from the scaled dot product attention.] \"\"\"\n",
        "        scores = torch.from_numpy(correlate(Q.detach().cpu().numpy(), K.detach().cpu().numpy(), mode='same'))\n",
        "        scores = scores.to(training_config.device)\n",
        "\n",
        "        scores = scores@K.transpose(-2, -1) / math.sqrt(self.d_k)\n",
        "\n",
        "        if mask is not None:\n",
        "            mask = mask.unsqueeze(1)\n",
        "            scores = scores.masked_fill(mask == 0, -1e9)\n",
        "\n",
        "        scores = F.softmax(scores, dim=-1)\n",
        "\n",
        "        if dropout is not None:\n",
        "            scores = dropout(scores)\n",
        "\n",
        "        output =  scores@V\n",
        "        return output\n",
        "\n",
        "    def _different_type_of_correlation(self, Q, K, V, mask=None, dropout=None):\n",
        "        raise NotImplementedError\n",
        "\n",
        "\n",
        "    def _another_different_type_of_correlation(self, Q, K, V, mask=None, dropout=None):\n",
        "        raise NotImplementedError\n",
        "\n",
        "\n",
        "    def split_heads(self, x):\n",
        "        batch_size, seq_length, d_model = x.size()\n",
        "        return x.view(batch_size, seq_length, self.num_heads, self.d_k).transpose(1, 2)\n",
        "\n",
        "\n",
        "    def combine_heads(self, x):\n",
        "        batch_size, _, seq_length, d_k = x.size()\n",
        "        return x.transpose(1, 2).contiguous().view(batch_size, seq_length, self.d_model)\n",
        "\n",
        "    def forward(self,x, mask=None):\n",
        "\n",
        "        Q = self.split_heads(self.W_q(x))\n",
        "        K = self.split_heads(self.W_k(x))\n",
        "        V = self.split_heads(self.W_v(x))\n",
        "\n",
        "        attn_output = self.scaled_correlation(Q, K, V, mask)\n",
        "        output = self.W_o(self.combine_heads(attn_output))\n",
        "\n",
        "        return output\n",
        "\n",
        "class Block(nn.Module):\n",
        "    \"\"\" This is where we can allow for creativity. Just that we can go from embed_dim, and back to embed_dim \"\"\"\n",
        "    def __init__(self, config) -> None:\n",
        "        super().__init__()\n",
        "        embed_dim = config.embed_dim\n",
        "\n",
        "        self.ln1 = nn.LayerNorm(embed_dim)\n",
        "        self.ln2 = nn.LayerNorm(embed_dim)\n",
        "\n",
        "        self.corr = MultiHeadedCorrelation(d_model = config.embed_dim, num_heads=config.num_heads)\n",
        "        self.ff = nn.Sequential(\n",
        "            nn.Linear(embed_dim, embed_dim*4),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(embed_dim*4, embed_dim),\n",
        "            nn.Dropout(config.ff_dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.corr(self.ln1(x))\n",
        "        x = x + self.ff(self.ln2(x))\n",
        "\n",
        "        return x    # This is the x which goes to the decoder."
      ],
      "metadata": {
        "id": "X5dHyE2Gz0_3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LeviathanComponentModel(nn.Module):\n",
        "    def __init__(self, config) -> None:\n",
        "        super().__init__()\n",
        "        embed_dim = config.embed_dim\n",
        "        self.max_len = config.max_len\n",
        "\n",
        "        self.tok_embed = nn.Embedding(config.vocab_size, embed_dim)\n",
        "\n",
        "        self.pos_embed = nn.Parameter(torch.zeros(1, config.max_len, embed_dim))\n",
        "\n",
        "        self.dropout = nn.Dropout(config.embed_dropout)\n",
        "\n",
        "        self.blocks = nn.Sequential(\n",
        "            *[Block(config) for _ in range(config.num_blocks)]\n",
        "        )\n",
        "\n",
        "        self.ln = nn.LayerNorm(embed_dim)\n",
        "        self.fc = nn.Linear(embed_dim, config.vocab_size)\n",
        "\n",
        "    def forward(self, x, target=None):\n",
        "        seq_len = x.size(1)\n",
        "\n",
        "        assert seq_len <= self.max_len, f\"Sequence Length longer than Model Capacity({seq_len}>{self.max_len}).\"\n",
        "\n",
        "        tok_embed = self.tok_embed(x)   # toke_embed.shape = (batch_size, seq_len, embed_dim)\n",
        "        pos_embed = self.pos_embed[:, :seq_len, :]  # pos_embed.shape = (1, seq_len, embed_dim)\n",
        "\n",
        "        x = self.dropout(tok_embed + pos_embed)\n",
        "        x = self.blocks(x)\n",
        "        x = self.ln(x)\n",
        "        logits = self.fc(x)   # x.shape = (batch_size, seq_len, vocab_size)\n",
        "\n",
        "        if target is None:\n",
        "            loss = None\n",
        "\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            target = target.view(B*T)\n",
        "            loss = F.cross_entropy(logits, target)\n",
        "\n",
        "        return logits, loss  # basically, token prediction per position.\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -training_config.block_size:]\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx"
      ],
      "metadata": {
        "id": "sCj4jSs5zIWc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Tokenizer:\n",
        "    \"\"\" Vanilla Tokenizer. Will train a BPE model and use that. \"\"\"\n",
        "    def __init__(self, text) -> None:\n",
        "        self._text = text\n",
        "        self._chars = sorted(list(set(self._text)))\n",
        "        self._vocab_size = len(self._chars)\n",
        "\n",
        "    def _string_to_integer(self):\n",
        "        \"\"\" Utility function for creating a mapping from character to integers. Converts string to integer. \"\"\"\n",
        "        return {ch: i for i, ch in enumerate(self._chars)}\n",
        "\n",
        "\n",
        "    def _integer_to_string(self):\n",
        "        \"\"\" Utility function for creating a mapping from integers to character. Converts integer to string. \"\"\"\n",
        "        return {ch: i for i, ch in enumerate(self._chars)}\n",
        "\n",
        "\n",
        "    def encode(self):\n",
        "        \"\"\" Encode the text, and returns the list of integers. \"\"\"\n",
        "        warnings.warn('This is a pretty naive methos to create a mapping, and will not be scalable once large enough dataset is there. ', DeprecationWarning)\n",
        "        return [self._string_to_integer(char) for char in self._text]\n",
        "\n",
        "\n",
        "    def decode(self, lst_ints:list[int]):\n",
        "        \"\"\" Decode the list of integers to convert them into string. Useful to see how the model understands things. \"\"\"\n",
        "        return ''.join([self._integer_to_string(i) for i in lst_ints])"
      ],
      "metadata": {
        "id": "8W93Z13Azg1I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.init()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 372,
          "referenced_widgets": [
            "c08e9d85bc044d669de2bdedf1c8f376",
            "bebcc0d9c09f46f0967f577e22e25344",
            "6826b435ddb94e139dc8a91d9b8dab85",
            "eee45f78dd264c7a8e2bd23b6fd4ca89",
            "173b2386cbab4a51b710e33f2e4aa5e3",
            "c60c5987fb3843f5808ae456e6abb7f8",
            "0e130898c25e4c5eb135ccffa70fd3f9",
            "97735ef7ff2c41ed9786057154790244"
          ]
        },
        "id": "IYRtdHap24zy",
        "outputId": "b7dbac07-74bf-44a0-ddbd-923921169df4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Finishing last run (ID:szy3j5ah) before initializing another..."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c08e9d85bc044d669de2bdedf1c8f376"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>step</td><td>▁▂▂▃▄▅▅▆▇▇█</td></tr><tr><td>train_loss</td><td>█▄▃▂▂▂▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▄▃▂▂▂▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>step</td><td>4999</td></tr><tr><td>train_loss</td><td>2.86838</td></tr><tr><td>val_loss</td><td>2.87245</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">pretty-elevator-5</strong> at: <a href='https://wandb.ai/ysrivastava82/uncategorized/runs/szy3j5ah' target=\"_blank\">https://wandb.ai/ysrivastava82/uncategorized/runs/szy3j5ah</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20240205_104415-szy3j5ah/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Successfully finished last run (ID:szy3j5ah). Initializing new run:<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.16.2"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20240205_112759-wg1bt8ld</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/ysrivastava82/uncategorized/runs/wg1bt8ld' target=\"_blank\">usual-salad-6</a></strong> to <a href='https://wandb.ai/ysrivastava82/uncategorized' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/ysrivastava82/uncategorized' target=\"_blank\">https://wandb.ai/ysrivastava82/uncategorized</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/ysrivastava82/uncategorized/runs/wg1bt8ld' target=\"_blank\">https://wandb.ai/ysrivastava82/uncategorized/runs/wg1bt8ld</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/ysrivastava82/uncategorized/runs/wg1bt8ld?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
            ],
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x7efe97bdd390>"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" Add the ability to track metrics and saving checkpoint, together with uploading them to HuggingFace. \"\"\"\n",
        "\n",
        "\n",
        "config = LeviathanModelConfig(vocab_size, max_len=1000)   ## Change this boi.\n",
        "model = LeviathanComponentModel(config)\n",
        "\n",
        "model = model.to(training_config.device)\n",
        "\n",
        "print(sum(p.numel() for p in model.parameters())/1e6, 'M parameters')\n",
        "\n",
        "wandb.watch(model, log_freq = training_config.eval_interval)\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=training_config.learning_rate)\n",
        "\n",
        "for iter in range(training_config.max_iters+1):\n",
        "    #if iter % save_internval == 0\n",
        "    if iter % training_config.eval_interval == 0 or iter == training_config.max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "        wandb.log({'step': iter, 'train_loss': losses['train'], 'val_loss': losses['val']})\n",
        "\n",
        "        # Uncomment to save the best models.\n",
        "\n",
        "        #torch.save({\n",
        "        #    'epoch': iter,\n",
        "        #    'model_state_dict': model.state_dict(),\n",
        "        #    'optimizer_state_dict': optimizer.state_dict(),\n",
        "        #    'loss': losses,\n",
        "        #    }, f'model_{iter}_{losses[\"train\"]:.4f}')\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xLFR7VuUyhey",
        "outputId": "90829ff7-3803-486e-e176-62c0ff8a824d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.005315 M parameters\n",
            "step 0: train loss 4.7157, val loss 4.7154\n",
            "step 100: train loss 4.4085, val loss 4.4033\n",
            "step 200: train loss 4.1525, val loss 4.1437\n",
            "step 300: train loss 3.9493, val loss 3.9378\n",
            "step 400: train loss 3.7849, val loss 3.7732\n",
            "step 500: train loss 3.6428, val loss 3.6279\n",
            "step 600: train loss 3.5239, val loss 3.5072\n",
            "step 700: train loss 3.4310, val loss 3.4116\n",
            "step 800: train loss 3.3599, val loss 3.3443\n",
            "step 900: train loss 3.3107, val loss 3.2895\n",
            "step 1000: train loss 3.2772, val loss 3.2591\n",
            "step 1100: train loss 3.2538, val loss 3.2394\n",
            "step 1200: train loss 3.2344, val loss 3.2156\n",
            "step 1300: train loss 3.2223, val loss 3.1985\n",
            "step 1400: train loss 3.2090, val loss 3.1879\n",
            "step 1500: train loss 3.1921, val loss 3.1742\n",
            "step 1600: train loss 3.1768, val loss 3.1590\n",
            "step 1700: train loss 3.1611, val loss 3.1417\n",
            "step 1800: train loss 3.1418, val loss 3.1197\n",
            "step 1900: train loss 3.1210, val loss 3.1026\n",
            "step 2000: train loss 3.0995, val loss 3.0849\n",
            "step 2100: train loss 3.0845, val loss 3.0666\n",
            "step 2200: train loss 3.0670, val loss 3.0503\n",
            "step 2300: train loss 3.0514, val loss 3.0352\n",
            "step 2400: train loss 3.0350, val loss 3.0193\n",
            "step 2500: train loss 3.0205, val loss 3.0057\n",
            "step 2600: train loss 3.0041, val loss 2.9920\n",
            "step 2700: train loss 2.9912, val loss 2.9762\n",
            "step 2800: train loss 2.9808, val loss 2.9657\n",
            "step 2900: train loss 2.9651, val loss 2.9532\n",
            "step 3000: train loss 2.9518, val loss 2.9351\n",
            "step 3100: train loss 2.9360, val loss 2.9317\n",
            "step 3200: train loss 2.9271, val loss 2.9185\n",
            "step 3300: train loss 2.9148, val loss 2.9059\n",
            "step 3400: train loss 2.9070, val loss 2.9006\n",
            "step 3500: train loss 2.8987, val loss 2.8927\n",
            "step 3600: train loss 2.8924, val loss 2.8870\n",
            "step 3700: train loss 2.8849, val loss 2.8816\n",
            "step 3800: train loss 2.8771, val loss 2.8778\n",
            "step 3900: train loss 2.8760, val loss 2.8693\n",
            "step 4000: train loss 2.8727, val loss 2.8669\n",
            "step 4100: train loss 2.8753, val loss 2.8692\n",
            "step 4200: train loss 2.8717, val loss 2.8689\n",
            "step 4300: train loss 2.8693, val loss 2.8674\n",
            "step 4400: train loss 2.8719, val loss 2.8672\n",
            "step 4500: train loss 2.8697, val loss 2.8615\n",
            "step 4600: train loss 2.8752, val loss 2.8673\n",
            "step 4700: train loss 2.8683, val loss 2.8640\n",
            "step 4800: train loss 2.8726, val loss 2.8705\n",
            "step 4900: train loss 2.8732, val loss 2.8705\n",
            "step 5000: train loss 2.8726, val loss 2.8735\n",
            "step 5100: train loss 2.8695, val loss 2.8688\n",
            "step 5200: train loss 2.8785, val loss 2.8796\n",
            "step 5300: train loss 2.8780, val loss 2.8753\n",
            "step 5400: train loss 2.8716, val loss 2.8714\n",
            "step 5500: train loss 2.8737, val loss 2.8738\n",
            "step 5600: train loss 2.8721, val loss 2.8726\n",
            "step 5700: train loss 2.8694, val loss 2.8685\n",
            "step 5800: train loss 2.8661, val loss 2.8647\n",
            "step 5900: train loss 2.8742, val loss 2.8752\n",
            "step 6000: train loss 2.8568, val loss 2.8552\n",
            "step 6100: train loss 2.8440, val loss 2.8487\n",
            "step 6200: train loss 2.8243, val loss 2.8262\n",
            "step 6300: train loss 2.8146, val loss 2.8155\n",
            "step 6400: train loss 2.8213, val loss 2.8288\n",
            "step 6500: train loss 2.8088, val loss 2.8151\n",
            "step 6600: train loss 2.8082, val loss 2.8133\n",
            "step 6700: train loss 2.7993, val loss 2.8029\n",
            "step 6800: train loss 2.7935, val loss 2.7959\n",
            "step 6900: train loss 2.7899, val loss 2.7953\n",
            "step 7000: train loss 2.7871, val loss 2.7937\n",
            "step 7100: train loss 2.7818, val loss 2.7873\n",
            "step 7200: train loss 2.7811, val loss 2.7864\n",
            "step 7300: train loss 2.7809, val loss 2.7843\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sKekNEKZ2iss"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}